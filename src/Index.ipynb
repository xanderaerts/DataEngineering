{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047cbc17",
   "metadata": {},
   "source": [
    "# ðŸŸ« Bronze Layer Data Pipeline with PySpark\n",
    "\n",
    "This repository contains a PySpark script that implements the **Bronze Layer** of a data lakehouse architecture. It ingests raw `.csv` files, adds metadata, handles corrupt records, and stores clean data in Parquet format.\n",
    "\n",
    "## ðŸ“ Directory Structure\n",
    "\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ Raw/        # Source CSV files (tab-delimited)\n",
    "â”œâ”€â”€ Bronze/     # Cleaned and structured data (Parquet)\n",
    "â”œâ”€â”€ Silver/     # [Reserved for further processing]\n",
    "â”œâ”€â”€ Gold/       # [Reserved for final curated datasets]\n",
    "logs/\n",
    "â””â”€â”€ bronze_layer_errors.log  # Log file for errors and warnings\n",
    "```\n",
    "\n",
    "## âš™ï¸ What the Script Does\n",
    "\n",
    "### 1. **Initialization**\n",
    "- Ensures all required directories exist (Raw, Bronze, Logs).\n",
    "- Sets up structured logging to both console and file.\n",
    "\n",
    "### 2. **Spark Session Setup**\n",
    "- Creates a Spark session with relevant options for timestamp parsing and error handling.\n",
    "- If the session fails, the script exits gracefully.\n",
    "\n",
    "### 3. **CSV Processing (Raw âž¡ï¸ Bronze)**\n",
    "For each `.csv` file in the `Raw/` folder:\n",
    "- Reads the file using tab (`\\t`) as the delimiter.\n",
    "- Infers schema and detects corrupt rows.\n",
    "- Adds an `ingestion_timestamp` column with the current timestamp.\n",
    "- Logs any corrupt records and saves them separately.\n",
    "- Writes the cleaned data as a Parquet file into `Bronze/` (one subfolder per file).\n",
    "- Stores each DataFrame in a dictionary for potential downstream use.\n",
    "\n",
    "### 4. **Schema Logging**\n",
    "- Logs the schema of the last processed DataFrame for verification.\n",
    "\n",
    "### 5. **Shutdown**\n",
    "- Stops the Spark session cleanly and logs the end of processing.\n",
    "\n",
    "## ðŸªµ Logging & Error Handling\n",
    "\n",
    "- All warnings and errors are saved to:\n",
    "  ```\n",
    "  logs/bronze_layer_errors.log\n",
    "  ```\n",
    "- Corrupt rows are automatically redirected to:\n",
    "  ```\n",
    "  logs/bad_records_from_bronze/\n",
    "  ```\n",
    "\n",
    "## ðŸ§ª Example Usage\n",
    "\n",
    "Simply drop one or more `.csv` files (tab-delimited) into the `data/Raw/` directory and run the script:\n",
    "\n",
    "```bash\n",
    "python bronze_layer.py\n",
    "```\n",
    "\n",
    "## ðŸ›  Requirements\n",
    "\n",
    "- Python 3.7+\n",
    "- PySpark\n",
    "- Local or distributed Spark environment\n",
    "\n",
    "## ðŸ“Œ Notes\n",
    "\n",
    "- Silver and Gold layers are defined for future use but not yet implemented.\n",
    "- The script is modular and can be extended for scheduling, validation, or transformation logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f4ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define paths for your data layers\n",
    "raw_dir = 'data/Raw/'\n",
    "bronze_dir = 'data/Bronze'\n",
    "silver_dir = 'data/Silver' # Defined for completeness, not used in Bronze layer logic yet\n",
    "gold_dir = 'data/Gold'     # Defined for completeness, not used in Bronze layer logic yet\n",
    "\n",
    "# Define path for the error log file\n",
    "error_log_dir = 'logs'\n",
    "error_log_file_path = os.path.join(error_log_dir, 'bronze_layer_errors.log')\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "os.makedirs(bronze_dir, exist_ok=True)\n",
    "os.makedirs(error_log_dir, exist_ok=True) # Ensure log directory exists\n",
    "\n",
    "# --- Logging Setup ---\n",
    "# Clear existing handlers to prevent duplicate logs if run multiple times in a notebook\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(error_log_file_path), # Log to the specified file\n",
    "        logging.StreamHandler()                    # Also print to console\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Starting PySpark Data Layer Processing (Bronze Layer)...\")\n",
    "\n",
    "# --- Spark Session Initialization ---\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"TakeHomeExamBronzeLayer\") \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .getOrCreate()\n",
    "    logger.info(\"SparkSession created successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing SparkSession: {e}\")\n",
    "    exit(1) # Exit if SparkSession cannot be created\n",
    "\n",
    "# --- Data Processing: Raw to Bronze Layer ---\n",
    "\n",
    "# Placeholder for a 'bad records' path if any malformed records are found during CSV read\n",
    "# This path will be within the 'logs' directory\n",
    "bad_records_path = os.path.join(error_log_dir, 'bad_records_from_bronze')\n",
    "os.makedirs(bad_records_path, exist_ok=True)\n",
    "\n",
    "file_names = [f for f in os.listdir(raw_dir) if f.endswith('.csv')]\n",
    "\n",
    "if not file_names:\n",
    "    logger.warning(f\"No CSV files found in the raw directory: {raw_dir}. Please ensure files are present.\")\n",
    "\n",
    "bronze_dfs = {} # Using a dictionary to store DFs by file name for easier access later\n",
    "\n",
    "for file_name in file_names:\n",
    "    raw_file_path = os.path.join(raw_dir, file_name)\n",
    "    # Remove .csv extension for the bronze folder name\n",
    "    bronze_output_path = os.path.join(bronze_dir, file_name.replace('.csv', ''))\n",
    "\n",
    "    logger.info(f\"Processing file: {file_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Read raw CSV with options for error handling and schema inference\n",
    "        bronze_df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"mode\", \"PERMISSIVE\") \\\n",
    "            .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "            .option(\"badRecordsPath\", bad_records_path) \\\n",
    "            .option(\"sep\", \"\\t\") \\\n",
    "            .csv(raw_file_path)\n",
    "        \n",
    "        # Add ingestion timestamp\n",
    "        bronze_df = bronze_df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "        # Check for corrupt records and log if any are found\n",
    "        if \"_corrupt_record\" in bronze_df.columns:\n",
    "            corrupt_count = bronze_df.filter(bronze_df[\"_corrupt_record\"].isNotNull()).count()\n",
    "            if corrupt_count > 0:\n",
    "                logger.warning(f\"Found {corrupt_count} corrupt records in {file_name}. Details in {bad_records_path}.\")\n",
    "            bronze_df = bronze_df.drop(\"_corrupt_record\") # Drop the corrupt record column after logging\n",
    "\n",
    "        # Write to Bronze layer in Parquet format\n",
    "        bronze_df.write.mode(\"overwrite\").parquet(bronze_output_path)\n",
    "        logger.info(f\"Successfully processed and wrote {file_name} to Bronze layer: {bronze_output_path}\")\n",
    "        \n",
    "        bronze_dfs[file_name.replace('.csv', '')] = bronze_df # Store DF by its new folder name\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# --- Schema Display (example for one DataFrame) ---\n",
    "if bronze_dfs:\n",
    "    # Get the key (folder name) of the last processed DataFrame\n",
    "    last_df_key = list(bronze_dfs.keys())[-1] \n",
    "    logger.info(f\"Schema of the last processed Bronze DataFrame ({last_df_key}):\")\n",
    "    bronze_dfs[last_df_key].printSchema()\n",
    "else:\n",
    "    logger.warning(\"No Bronze DataFrames were created.\")\n",
    "\n",
    "logger.info(\"Bronze Layer processing finished.\")\n",
    "\n",
    "# --- Stop Spark Session ---\n",
    "spark.stop()\n",
    "logger.info(\"SparkSession stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ee3e3",
   "metadata": {},
   "source": [
    "# Silver layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5527ca",
   "metadata": {},
   "source": [
    "# ðŸ¥ˆ Silver Layer Data Pipeline with PySpark\n",
    "\n",
    "This repository contains a PySpark script that processes the **Silver Layer** in a data lakehouse architecture. It refines raw data ingested into the Bronze Layer by performing validation, cleaning, standardization, and enhancement of data quality.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‚ Directory Structure\n",
    "\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ Bronze/         # Cleaned data from raw (input to Silver layer)\n",
    "â”œâ”€â”€ Silver/         # Refined, validated data (output from Silver layer)\n",
    "logs/\n",
    "â””â”€â”€ silver_layer_errors.log  # Log file for errors, warnings, and stats\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ What the Script Does\n",
    "\n",
    "### ðŸ” Main Flow\n",
    "\n",
    "1. **Initial Setup**\n",
    "   - Creates required directories (`Silver/`, `logs/`)\n",
    "   - Sets up logging to file and console\n",
    "   - Starts a SparkSession\n",
    "\n",
    "2. **Table-Specific Processing**\n",
    "   - Reads Parquet files from `Bronze/`\n",
    "   - Cleans string fields (e.g. trimming, null handling)\n",
    "   - Validates fields: dates, numerics, booleans, business rules\n",
    "   - Adds metadata like `processing_timestamp`, `data_quality_score`\n",
    "   - Applies transformations and writes back to `Silver/` as Parquet\n",
    "\n",
    "3. **Tables Covered**\n",
    "   - `address`\n",
    "   - `customer`\n",
    "   - `person`\n",
    "   - `product`\n",
    "   - `salesorderheader`\n",
    "   - `salesorderdetail`\n",
    "   - `creditcard`\n",
    "   - Any other table â†’ handled by a generic processor\n",
    "\n",
    "4. **Data Quality Checks**\n",
    "   - Null percentage calculation per column\n",
    "   - Logical validations like:\n",
    "     - Prices (e.g. `ListPrice >= StandardCost`)\n",
    "     - Monetary totals (`TotalDue = SubTotal + Tax + Freight`)\n",
    "     - Date sequences (e.g. `OrderDate <= ShipDate`)\n",
    "     - Card masking (e.g. `****-****-****-1234`)\n",
    "\n",
    "5. **Logs Summary**\n",
    "   - Full DQ stats (record count changes, null percentages)\n",
    "   - Errors are caught and logged per table\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Key Validation Examples\n",
    "\n",
    "| Validation Type      | Example Columns               | Logic |\n",
    "|----------------------|-------------------------------|-------|\n",
    "| **Boolean cleanup**  | `MakeFlag`, `OnlineOrderFlag` | Normalize 1/0, true/false, yes/no |\n",
    "| **Date parsing**     | `ModifiedDate`, `OrderDate`   | Convert to `DateType`, check sequence |\n",
    "| **Numeric check**    | `ListPrice`, `StandardCost`   | Ensure non-negative |\n",
    "| **Business logic**   | `LineTotal`, `TotalDue`       | Match expected formulas |\n",
    "| **Text cleaning**    | `Name`, `Address`             | Trim, set empty to null |\n",
    "| **Card masking**     | `CardNumber`                  | Format to `****-****-****-1234` |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ How to Run\n",
    "\n",
    "```bash\n",
    "python silver_layer.py\n",
    "```\n",
    "\n",
    "Make sure Bronze layer data is already prepared in `data/Bronze/` as Parquet files.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Output Example\n",
    "\n",
    "For each table, a `.parquet` folder will be created in `data/Silver/`, containing validated data.\n",
    "\n",
    "Logging will summarize:\n",
    "- Number of records in vs. out\n",
    "- % of nulls per column\n",
    "- Any high-null columns (>20%) flagged\n",
    "- Any failed validations\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Notes\n",
    "\n",
    "- Designed for modular extension to Gold Layer and analytics\n",
    "- Validation logic is centralized in helper functions\n",
    "- Each table is processed independently to isolate issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b0dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    current_timestamp, lit, col, when, isnan, isnull, trim, upper, lower,\n",
    "    regexp_replace, to_date, to_timestamp, coalesce, length, substring,\n",
    "    round as spark_round, abs as spark_abs, split, size, concat, initcap\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType, BooleanType\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define paths for your data layers\n",
    "bronze_dir = 'data/Bronze'\n",
    "silver_dir = 'data/Silver'\n",
    "gold_dir = 'data/Gold'     # Defined for completeness, not used in Silver layer logic yet\n",
    "\n",
    "# Define path for the error log file\n",
    "error_log_dir = 'logs'\n",
    "error_log_file_path = os.path.join(error_log_dir, 'silver_layer_errors.log')\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(silver_dir, exist_ok=True)\n",
    "os.makedirs(error_log_dir, exist_ok=True)\n",
    "\n",
    "# --- Logging Setup ---\n",
    "# Clear existing handlers to prevent duplicate logs\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(error_log_file_path),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Starting PySpark Data Layer Processing (Silver Layer)...\")\n",
    "\n",
    "# --- Spark Session Initialization ---\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"TakeHomeExamSilverLayer\") \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .getOrCreate()\n",
    "    logger.info(\"SparkSession created successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing SparkSession: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- Data Quality Functions ---\n",
    "def clean_string_column(df, column_name):\n",
    "    \"\"\"Clean string columns by trimming whitespace and handling nulls\"\"\"\n",
    "    if column_name in df.columns:\n",
    "        return df.withColumn(column_name, \n",
    "            when(col(column_name).isNull() | (trim(col(column_name)) == \"\"), None)\n",
    "            .otherwise(trim(col(column_name)))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def standardize_boolean_flags(df, column_name):\n",
    "    \"\"\"Standardize boolean flags to consistent format\"\"\"\n",
    "    if column_name in df.columns:\n",
    "        # Cast the column to StringType to handle mixed types gracefully\n",
    "        # Then convert to boolean based on various string representations\n",
    "        return df.withColumn(column_name,\n",
    "            when(lower(col(column_name).cast(StringType())).isin([\"1\", \"true\", \"t\", \"y\", \"yes\"]), True)\n",
    "            .when(lower(col(column_name).cast(StringType())).isin([\"0\", \"false\", \"f\", \"n\", \"no\"]), False)\n",
    "            .otherwise(None)\n",
    "        ).withColumn(column_name, col(column_name).cast(BooleanType())) # Ensure final type is Boolean\n",
    "    return df\n",
    "\n",
    "def validate_dates(df, column_name):\n",
    "    \"\"\"Validate and clean date columns\"\"\"\n",
    "    if column_name in df.columns:\n",
    "        return df.withColumn(column_name,\n",
    "            when(col(column_name).isNull(), None)\n",
    "            .otherwise(to_date(col(column_name)))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def validate_numeric_positive(df, column_name):\n",
    "    \"\"\"Ensure numeric columns are positive where applicable\"\"\"\n",
    "    if column_name in df.columns:\n",
    "        return df.withColumn(column_name,\n",
    "            when(col(column_name) < 0, None)\n",
    "            .otherwise(col(column_name))\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def add_data_quality_flags(df, table_name):\n",
    "    \"\"\"Add data quality flags for monitoring\"\"\"\n",
    "    df = df.withColumn(\"data_quality_score\", lit(1.0))\n",
    "    df = df.withColumn(\"source_table\", lit(table_name))\n",
    "    df = df.withColumn(\"processing_timestamp\", current_timestamp())\n",
    "    return df\n",
    "\n",
    "# --- Silver Layer Processing Functions ---\n",
    "\n",
    "def process_address(bronze_df):\n",
    "    \"\"\"Process address table with data quality checks\"\"\"\n",
    "    logger.info(\"Processing address table...\")\n",
    "    \n",
    "    df = bronze_df\n",
    "    \n",
    "    # Clean string columns\n",
    "    string_cols = ['AddressLine1', 'AddressLine2', 'City', 'PostalCode']\n",
    "    for col_name in string_cols:\n",
    "        df = clean_string_column(df, col_name)\n",
    "    \n",
    "    # Validate postal codes (basic format check)\n",
    "    if 'PostalCode' in df.columns:\n",
    "        df = df.withColumn('PostalCode',\n",
    "            when(length(col('PostalCode')) < 3, None)\n",
    "            .otherwise(col('PostalCode'))\n",
    "        )\n",
    "    \n",
    "    # Validate dates\n",
    "    df = validate_dates(df, 'ModifiedDate')\n",
    "    \n",
    "    # Add quality flags\n",
    "    df = add_data_quality_flags(df, 'address')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_customer(bronze_df):\n",
    "    \"\"\"Process customer table\"\"\"\n",
    "    logger.info(\"Processing customer table...\")\n",
    "    \n",
    "    df = bronze_df\n",
    "    \n",
    "    # Clean account number\n",
    "    df = clean_string_column(df, 'AccountNumber')\n",
    "    \n",
    "    # Validate IDs are positive\n",
    "    id_cols = ['CustomerID', 'PersonID', 'StoreID', 'TerritoryID']\n",
    "    for col_name in id_cols:\n",
    "        df = validate_numeric_positive(df, col_name)\n",
    "    \n",
    "    # Validate dates\n",
    "    df = validate_dates(df, 'ModifiedDate')\n",
    "    \n",
    "    df = add_data_quality_flags(df, 'customer')\n",
    "    return df\n",
    "\n",
    "def process_person(bronze_df):\n",
    "    \"\"\"Process person table with name standardization\"\"\"\n",
    "    logger.info(\"Processing person table...\")\n",
    "\n",
    "    df = bronze_df\n",
    "\n",
    "    # Clean and standardize name fields\n",
    "    name_cols = ['FirstName', 'MiddleName', 'LastName', 'Title', 'Suffix']\n",
    "    for col_name in name_cols:\n",
    "        df = clean_string_column(df, col_name)\n",
    "        if col_name in df.columns:\n",
    "            # Capitalize first letter of each word using initcap()\n",
    "            df = df.withColumn(col_name,\n",
    "                when(col(col_name).isNotNull(), initcap(col(col_name)))\n",
    "                .otherwise(None)\n",
    "            )\n",
    "\n",
    "    # Standardize PersonType\n",
    "    df = clean_string_column(df, 'PersonType')\n",
    "\n",
    "    # Validate EmailPromotion values\n",
    "    if 'EmailPromotion' in df.columns:\n",
    "        df = df.withColumn('EmailPromotion',\n",
    "            when(col('EmailPromotion').isin([0, 1, 2]), col('EmailPromotion'))\n",
    "            .otherwise(0)\n",
    "        )\n",
    "\n",
    "    # Validate and clean date fields\n",
    "    df = validate_dates(df, 'ModifiedDate')\n",
    "\n",
    "    # Add DQ flags\n",
    "    df = add_data_quality_flags(df, 'person')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_product(bronze_df):\n",
    "    \"\"\"Process product table with comprehensive validation\"\"\"\n",
    "    logger.info(\"Processing product table...\")\n",
    "    \n",
    "    df = bronze_df\n",
    "    \n",
    "    # Clean string columns\n",
    "    string_cols = ['Name', 'ProductNumber', 'Color', 'Size', 'ProductLine', 'Class', 'Style']\n",
    "    for col_name in string_cols:\n",
    "        df = clean_string_column(df, col_name)\n",
    "    \n",
    "    # Standardize boolean flags\n",
    "    bool_cols = ['MakeFlag', 'FinishedGoodsFlag']\n",
    "    for col_name in bool_cols:\n",
    "        df = standardize_boolean_flags(df, col_name)\n",
    "    \n",
    "    # Validate numeric columns\n",
    "    numeric_cols = ['SafetyStockLevel', 'ReorderPoint', 'StandardCost', 'ListPrice', 'Weight', 'DaysToManufacture']\n",
    "    for col_name in numeric_cols:\n",
    "        df = validate_numeric_positive(df, col_name)\n",
    "    \n",
    "    # Validate price consistency (ListPrice >= StandardCost)\n",
    "    if 'ListPrice' in df.columns and 'StandardCost' in df.columns:\n",
    "        df = df.withColumn('price_validation_flag',\n",
    "            when((col('ListPrice').isNotNull() & col('StandardCost').isNotNull()),\n",
    "                col('ListPrice') >= col('StandardCost')\n",
    "            ).otherwise(True)\n",
    "        )\n",
    "    \n",
    "    # Validate dates\n",
    "    date_cols = ['SellStartDate', 'SellEndDate', 'DiscontinuedDate', 'ModifiedDate']\n",
    "    for col_name in date_cols:\n",
    "        df = validate_dates(df, col_name)\n",
    "    \n",
    "    df = add_data_quality_flags(df, 'product')\n",
    "    return df\n",
    "\n",
    "def process_sales_order_header(bronze_df):\n",
    "    \"\"\"Process sales order header with business logic validation\"\"\"\n",
    "    logger.info(\"Processing salesorderheader table...\")\n",
    "    \n",
    "    df = bronze_df\n",
    "    \n",
    "    # Clean string columns\n",
    "    string_cols = ['SalesOrderNumber', 'PurchaseOrderNumber', 'AccountNumber', 'Comment']\n",
    "    for col_name in string_cols:\n",
    "        df = clean_string_column(df, col_name)\n",
    "    \n",
    "    # Standardize boolean flags\n",
    "    df = standardize_boolean_flags(df, 'OnlineOrderFlag')\n",
    "    \n",
    "    # Validate dates and date logic\n",
    "    date_cols = ['OrderDate', 'DueDate', 'ShipDate', 'ModifiedDate']\n",
    "    for col_name in date_cols:\n",
    "        df = validate_dates(df, col_name)\n",
    "    \n",
    "    # Validate date sequence (OrderDate <= DueDate, OrderDate <= ShipDate)\n",
    "    df = df.withColumn('date_sequence_valid',\n",
    "        when(\n",
    "            (col('OrderDate').isNotNull() & col('DueDate').isNotNull()),\n",
    "            col('OrderDate') <= col('DueDate')\n",
    "        ).otherwise(True) &\n",
    "        when(\n",
    "            (col('OrderDate').isNotNull() & col('ShipDate').isNotNull()),\n",
    "            col('OrderDate') <= col('ShipDate')\n",
    "        ).otherwise(True)\n",
    "    )\n",
    "    \n",
    "    # Validate monetary amounts\n",
    "    money_cols = ['SubTotal', 'TaxAmt', 'Freight', 'TotalDue']\n",
    "    for col_name in money_cols:\n",
    "        df = validate_numeric_positive(df, col_name)\n",
    "    \n",
    "    # Validate total calculation (TotalDue = SubTotal + TaxAmt + Freight)\n",
    "    if all(col_name in df.columns for col_name in ['SubTotal', 'TaxAmt', 'Freight', 'TotalDue']):\n",
    "        df = df.withColumn('total_calculation_valid',\n",
    "            when(\n",
    "                col('SubTotal').isNotNull() & \n",
    "                col('TaxAmt').isNotNull() & \n",
    "                col('Freight').isNotNull() &\n",
    "                col('TotalDue').isNotNull(),\n",
    "                spark_abs(col('TotalDue') - (col('SubTotal') + col('TaxAmt') + col('Freight'))) < 0.01\n",
    "            ).otherwise(True)\n",
    "        )\n",
    "    \n",
    "    df = add_data_quality_flags(df, 'salesorderheader')\n",
    "    return df\n",
    "\n",
    "def process_sales_order_detail(bronze_df):\n",
    "    \"\"\"Process sales order detail with calculation validation\"\"\"\n",
    "    logger.info(\"Processing salesorderdetail table...\")\n",
    "    \n",
    "    df = bronze_df\n",
    "    \n",
    "    # Clean string columns\n",
    "    df = clean_string_column(df, 'CarrierTrackingNumber')\n",
    "    \n",
    "    # Validate numeric columns\n",
    "    numeric_cols = ['OrderQty', 'UnitPrice', 'UnitPriceDiscount', 'LineTotal']\n",
    "    for col_name in numeric_cols:\n",
    "        df = validate_numeric_positive(df, col_name)\n",
    "    \n",
    "    # Validate line total calculation\n",
    "    if all(col_name in df.columns for col_name in ['OrderQty', 'UnitPrice', 'UnitPriceDiscount', 'LineTotal']):\n",
    "        df = df.withColumn('line_total_valid',\n",
    "            when(\n",
    "                col('OrderQty').isNotNull() & \n",
    "                col('UnitPrice').isNotNull() & \n",
    "                col('UnitPriceDiscount').isNotNull() &\n",
    "                col('LineTotal').isNotNull(),\n",
    "                spark_abs(col('LineTotal') - (col('OrderQty') * col('UnitPrice') * (1 - col('UnitPriceDiscount')))) < 0.01\n",
    "            ).otherwise(True)\n",
    "        )\n",
    "    \n",
    "    df = validate_dates(df, 'ModifiedDate')\n",
    "    df = add_data_quality_flags(df, 'salesorderdetail')\n",
    "    return df\n",
    "\n",
    "def process_creditcard(bronze_df):\n",
    "    \"\"\"Process credit card with sensitive data handling\"\"\"\n",
    "    logger.info(\"Processing creditcard table...\")\n",
    "    \n",
    "    df = bronze_df\n",
    "    \n",
    "    # Clean card type\n",
    "    df = clean_string_column(df, 'CardType')\n",
    "    \n",
    "    # Mask credit card number (keep last 4 digits)\n",
    "    if 'CardNumber' in df.columns:\n",
    "        df = df.withColumn('CardNumber',\n",
    "            when(col('CardNumber').isNotNull() & (length(col('CardNumber')) >= 4),\n",
    "                concat(lit(\"****-****-****-\"), substring(col('CardNumber'), -4, 4))\n",
    "            ).otherwise(lit(\"****-****-****-****\"))\n",
    "        )\n",
    "    \n",
    "    # Validate expiration dates\n",
    "    if 'ExpMonth' in df.columns:\n",
    "        df = df.withColumn('ExpMonth',\n",
    "            when((col('ExpMonth') >= 1) & (col('ExpMonth') <= 12), col('ExpMonth'))\n",
    "            .otherwise(None)\n",
    "        )\n",
    "    \n",
    "    if 'ExpYear' in df.columns:\n",
    "        df = df.withColumn('ExpYear',\n",
    "            when((col('ExpYear') >= 2020) & (col('ExpYear') <= 2050), col('ExpYear'))\n",
    "            .otherwise(None)\n",
    "        )\n",
    "    \n",
    "    df = validate_dates(df, 'ModifiedDate')\n",
    "    df = add_data_quality_flags(df, 'creditcard')\n",
    "    return df\n",
    "\n",
    "def process_generic_table(bronze_df, table_name):\n",
    "    \"\"\"Generic processing for simpler tables\"\"\"\n",
    "    logger.info(f\"Processing {table_name} table...\")\n",
    "    \n",
    "    df = bronze_df\n",
    "    \n",
    "    # Clean all string columns\n",
    "    for col_name, col_type in df.dtypes:\n",
    "        if col_type == 'string':\n",
    "            df = clean_string_column(df, col_name)\n",
    "    \n",
    "    # Validate ModifiedDate if exists\n",
    "    df = validate_dates(df, 'ModifiedDate')\n",
    "    \n",
    "    # Add quality flags\n",
    "    df = add_data_quality_flags(df, table_name)\n",
    "    return df\n",
    "\n",
    "# --- Main Processing Logic ---\n",
    "\n",
    "# Dictionary mapping table names to their specific processing functions\n",
    "table_processors = {\n",
    "    'address': process_address,\n",
    "    'customer': process_customer,\n",
    "    'person': process_person,\n",
    "    'product': process_product,\n",
    "    'salesorderheader': process_sales_order_header,\n",
    "    'salesorderdetail': process_sales_order_detail,\n",
    "    'creditcard': process_creditcard\n",
    "}\n",
    "\n",
    "# Get list of Bronze layer tables\n",
    "bronze_tables = [d for d in os.listdir(bronze_dir) if os.path.isdir(os.path.join(bronze_dir, d))]\n",
    "\n",
    "if not bronze_tables:\n",
    "    logger.warning(f\"No Bronze tables found in directory: {bronze_dir}\")\n",
    "    exit(1)\n",
    "\n",
    "silver_dfs = {}\n",
    "processing_stats = {}\n",
    "\n",
    "for table_name in bronze_tables:\n",
    "    bronze_table_path = os.path.join(bronze_dir, table_name)\n",
    "    silver_table_path = os.path.join(silver_dir, table_name)\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Processing Bronze table: {table_name}\")\n",
    "        \n",
    "        # Read Bronze layer data\n",
    "        bronze_df = spark.read.parquet(bronze_table_path)\n",
    "        \n",
    "        # Get initial record count\n",
    "        initial_count = bronze_df.count()\n",
    "        \n",
    "        # Apply specific processing function or generic processing\n",
    "        if table_name in table_processors:\n",
    "            silver_df = table_processors[table_name](bronze_df)\n",
    "        else:\n",
    "            silver_df = process_generic_table(bronze_df, table_name)\n",
    "        \n",
    "        # Get final record count\n",
    "        final_count = silver_df.count()\n",
    "        \n",
    "        # Calculate null percentages for key columns\n",
    "        total_cols = len(silver_df.columns)\n",
    "        null_counts = {}\n",
    "        for col_name in silver_df.columns:\n",
    "            if col_name not in ['ingestion_timestamp', 'processing_timestamp', 'data_quality_score', 'source_table']:\n",
    "                null_count = silver_df.filter(col(col_name).isNull()).count()\n",
    "                null_percentage = (null_count / final_count * 100) if final_count > 0 else 0\n",
    "                null_counts[col_name] = null_percentage\n",
    "        \n",
    "        # Store processing statistics\n",
    "        processing_stats[table_name] = {\n",
    "            'initial_count': initial_count,\n",
    "            'final_count': final_count,\n",
    "            'records_dropped': initial_count - final_count,\n",
    "            'null_percentages': null_counts\n",
    "        }\n",
    "        \n",
    "        # Write to Silver layer\n",
    "        silver_df.write.mode(\"overwrite\").parquet(silver_table_path)\n",
    "        logger.info(f\"Successfully processed {table_name}: {initial_count} -> {final_count} records\")\n",
    "        \n",
    "        # Store DataFrame for potential further use\n",
    "        silver_dfs[table_name] = silver_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing {table_name}: {e}\")\n",
    "        processing_stats[table_name] = {'error': str(e)}\n",
    "\n",
    "# --- Data Quality Report ---\n",
    "logger.info(\"=== SILVER LAYER DATA QUALITY REPORT ===\")\n",
    "for table_name, stats in processing_stats.items():\n",
    "    if 'error' in stats:\n",
    "        logger.error(f\"{table_name}: Processing failed - {stats['error']}\")\n",
    "    else:\n",
    "        logger.info(f\"{table_name}:\")\n",
    "        logger.info(f\"  Records: {stats['initial_count']} -> {stats['final_count']}\")\n",
    "        if stats['records_dropped'] > 0:\n",
    "            logger.warning(f\"  Dropped records: {stats['records_dropped']}\")\n",
    "        \n",
    "        # Report columns with high null percentages\n",
    "        high_null_cols = {k: v for k, v in stats['null_percentages'].items() if v > 20}\n",
    "        if high_null_cols:\n",
    "            logger.warning(f\"  High null percentage columns: {high_null_cols}\")\n",
    "\n",
    "# --- Schema Display (example for one DataFrame) ---\n",
    "if silver_dfs:\n",
    "    sample_table = list(silver_dfs.keys())[0]\n",
    "    logger.info(f\"Sample Silver DataFrame schema ({sample_table}):\")\n",
    "    silver_dfs[sample_table].printSchema()\n",
    "\n",
    "logger.info(\"Silver Layer processing finished.\")\n",
    "\n",
    "# --- Stop Spark Session ---\n",
    "spark.stop()\n",
    "logger.info(\"SparkSession stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa785936",
   "metadata": {},
   "source": [
    "# Gold layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    current_timestamp, lit, col, when, isnan, isnull, trim, upper, lower,\n",
    "    regexp_replace, to_date, to_timestamp, coalesce, length, substring,\n",
    "    round as spark_round, abs as spark_abs, split, size, concat, sum as spark_sum,\n",
    "    count, max as spark_max, min as spark_min, avg, year, month, dayofmonth,\n",
    "    dayofweek, quarter, weekofyear, date_format, row_number, rank, dense_rank,\n",
    "    lag, lead, first, last, collect_list, collect_set, explode, array_contains,\n",
    "    struct, desc, asc, monotonically_increasing_id, hash, md5, sha1,\n",
    "    regexp_extract, split as spark_split, slice as spark_slice,\n",
    "    date_add, date_sub, datediff, months_between, next_day, last_day,\n",
    "    from_unixtime, unix_timestamp, date_trunc, concat_ws\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType, BooleanType, LongType\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# --- Configuration ---\n",
    "silver_dir = 'data/Silver'\n",
    "gold_dir = 'data/Gold'\n",
    "\n",
    "# Define path for the error log file\n",
    "error_log_dir = 'logs'\n",
    "error_log_file_path = os.path.join(error_log_dir, 'gold_layer_errors.log')\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(gold_dir, exist_ok=True)\n",
    "os.makedirs(error_log_dir, exist_ok=True)\n",
    "\n",
    "# --- Logging Setup ---\n",
    "# Clear existing handlers to prevent duplicate logs\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(error_log_file_path),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Starting PySpark Data Layer Processing (Gold Layer with SCD and Upsert)...\")\n",
    "\n",
    "# --- Spark Session Initialization ---\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"TakeHomeExamGoldLayer\") \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    logger.info(\"SparkSession created successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error initializing SparkSession: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "def generate_surrogate_key(df, columns):\n",
    "    \"\"\"Generate surrogate key based on multiple columns\"\"\"\n",
    "    concat_cols = [coalesce(col(c).cast(StringType()), lit(\"NULL\")) for c in columns]\n",
    "    return df.withColumn(\"surrogate_key\",\n",
    "                         spark_abs(hash(concat(*concat_cols))))\n",
    "\n",
    "def safe_table_read(table_path, table_name):\n",
    "    \"\"\"Safely read a table with error handling\"\"\"\n",
    "    try:\n",
    "        return spark.read.parquet(table_path)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not read {table_name} from {table_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def add_scd_columns(df):\n",
    "    \"\"\"Add SCD Type 2 columns to dimension tables\"\"\"\n",
    "    return df.withColumn(\"effective_date\", current_timestamp()) \\\n",
    "             .withColumn(\"end_date\", lit(None).cast(TimestampType())) \\\n",
    "             .withColumn(\"is_current\", lit(True)) \\\n",
    "             .withColumn(\"version\", lit(1))\n",
    "\n",
    "def upsert_dimension_table(new_data, table_path, table_name, business_key_columns):\n",
    "    \"\"\"\n",
    "    Perform SCD Type 2 upsert on dimension table\n",
    "    \"\"\"\n",
    "    logger.info(f\"Performing SCD Type 2 upsert for {table_name}...\")\n",
    "    \n",
    "    # Check if table exists\n",
    "    if os.path.exists(table_path):\n",
    "        try:\n",
    "            existing_data = spark.read.parquet(table_path)\n",
    "            logger.info(f\"Found existing {table_name} with {existing_data.count()} records\")\n",
    "            \n",
    "            # Get current records (is_current = True)\n",
    "            current_records = existing_data.filter(col(\"is_current\") == True)\n",
    "            \n",
    "            # Create a comparison key for detecting changes\n",
    "            comparison_cols = [c for c in new_data.columns if c not in ['created_date', 'modified_date', 'effective_date', 'end_date', 'is_current', 'version', 'surrogate_key']]\n",
    "            \n",
    "            # Join to find changes\n",
    "            joined_df = new_data.alias(\"new\").join(\n",
    "                current_records.alias(\"existing\"),\n",
    "                [col(f\"new.{bk}\") == col(f\"existing.{bk}\") for bk in business_key_columns],\n",
    "                \"left\"\n",
    "            )\n",
    "            \n",
    "            # Identify new records (no match in existing)\n",
    "            new_records = joined_df.filter(col(\"existing.surrogate_key\").isNull()) \\\n",
    "                                   .select(\"new.*\")\n",
    "            \n",
    "            # Identify changed records\n",
    "            changed_records = joined_df.filter(col(\"existing.surrogate_key\").isNotNull())\n",
    "            \n",
    "            # Check for actual changes in data\n",
    "            change_conditions = []\n",
    "            for col_name in comparison_cols:\n",
    "                if col_name in business_key_columns:\n",
    "                    continue\n",
    "                change_conditions.append(\n",
    "                    coalesce(col(f\"new.{col_name}\"), lit(\"NULL\")) != \n",
    "                    coalesce(col(f\"existing.{col_name}\"), lit(\"NULL\"))\n",
    "                )\n",
    "            \n",
    "            if change_conditions:\n",
    "                changed_filter = change_conditions[0]\n",
    "                for condition in change_conditions[1:]:\n",
    "                    changed_filter = changed_filter | condition\n",
    "                \n",
    "                actually_changed = changed_records.filter(changed_filter)\n",
    "                unchanged = changed_records.filter(~changed_filter)\n",
    "            else:\n",
    "                actually_changed = spark.createDataFrame([], new_data.schema)\n",
    "                unchanged = changed_records\n",
    "            \n",
    "            # Handle unchanged records - keep existing\n",
    "            unchanged_existing = unchanged.select(\"existing.*\")\n",
    "            \n",
    "            # Handle new records - add with SCD columns\n",
    "            final_new_records = add_scd_columns(new_records)\n",
    "            \n",
    "            # Handle changed records\n",
    "            if actually_changed.count() > 0:\n",
    "                # Close existing records\n",
    "                closed_records = actually_changed.select(\"existing.*\") \\\n",
    "                    .withColumn(\"end_date\", current_timestamp()) \\\n",
    "                    .withColumn(\"is_current\", lit(False))\n",
    "                \n",
    "                # Create new versions of changed records\n",
    "                new_versions = actually_changed.select(\"new.*\") \\\n",
    "                    .join(actually_changed.select(\"existing.version\", *[f\"existing.{bk}\" for bk in business_key_columns]),\n",
    "                          [col(f\"new.{bk}\") == col(f\"existing.{bk}\") for bk in business_key_columns]) \\\n",
    "                    .withColumn(\"effective_date\", current_timestamp()) \\\n",
    "                    .withColumn(\"end_date\", lit(None).cast(TimestampType())) \\\n",
    "                    .withColumn(\"is_current\", lit(True)) \\\n",
    "                    .withColumn(\"version\", col(\"existing.version\") + 1) \\\n",
    "                    .drop(*[f\"existing.{bk}\" for bk in business_key_columns]) \\\n",
    "                    .drop(\"existing.version\")\n",
    "                \n",
    "                # Combine all records\n",
    "                historical_records = existing_data.filter(col(\"is_current\") == False)\n",
    "                result_df = historical_records.union(unchanged_existing) \\\n",
    "                                             .union(closed_records) \\\n",
    "                                             .union(final_new_records) \\\n",
    "                                             .union(new_versions)\n",
    "            else:\n",
    "                # No changes, just add new records\n",
    "                historical_records = existing_data.filter(col(\"is_current\") == False)\n",
    "                result_df = historical_records.union(unchanged_existing) \\\n",
    "                                             .union(final_new_records)\n",
    "                \n",
    "            logger.info(f\"SCD upsert completed: {result_df.count()} total records\")\n",
    "            return result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during SCD upsert for {table_name}: {e}\")\n",
    "            # Fall back to initial load\n",
    "            return add_scd_columns(new_data)\n",
    "    else:\n",
    "        # Initial load - add SCD columns\n",
    "        logger.info(f\"Initial load for {table_name}\")\n",
    "        return add_scd_columns(new_data)\n",
    "\n",
    "def upsert_fact_table(new_data, table_path, table_name, business_key_columns):\n",
    "    \"\"\"\n",
    "    Perform upsert on fact table (Type 1 - overwrite existing records)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Performing fact table upsert for {table_name}...\")\n",
    "    \n",
    "    if os.path.exists(table_path):\n",
    "        try:\n",
    "            existing_data = spark.read.parquet(table_path)\n",
    "            logger.info(f\"Found existing {table_name} with {existing_data.count()} records\")\n",
    "            \n",
    "            # For fact tables, we typically do a merge (upsert)\n",
    "            # Remove existing records that match business keys\n",
    "            existing_to_keep = existing_data.alias(\"existing\").join(\n",
    "                new_data.alias(\"new\"),\n",
    "                [col(f\"existing.{bk}\") == col(f\"new.{bk}\") for bk in business_key_columns],\n",
    "                \"left_anti\"\n",
    "            )\n",
    "            \n",
    "            # Combine with new data\n",
    "            result_df = existing_to_keep.union(new_data)\n",
    "            \n",
    "            logger.info(f\"Fact upsert completed: {result_df.count()} total records\")\n",
    "            return result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during fact upsert for {table_name}: {e}\")\n",
    "            return new_data\n",
    "    else:\n",
    "        logger.info(f\"Initial load for fact table {table_name}\")\n",
    "        return new_data\n",
    "\n",
    "def create_date_dimension():\n",
    "    \"\"\"Create comprehensive date dimension table\"\"\"\n",
    "    logger.info(\"Creating date dimension...\")\n",
    "\n",
    "    # Create date range from 2010 to 2030\n",
    "    start_date = date(2010, 1, 1)\n",
    "    end_date = date(2030, 12, 31)\n",
    "\n",
    "    # Generate date range\n",
    "    date_list = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        date_list.append((current_date,))\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    schema = StructType([StructField(\"date\", DateType(), True)])\n",
    "    date_df = spark.createDataFrame(date_list, schema)\n",
    "\n",
    "    # Add date dimension attributes\n",
    "    date_dim = date_df.select(\n",
    "        col(\"date\").alias(\"date_key\"),\n",
    "        col(\"date\"),\n",
    "        year(\"date\").alias(\"year\"),\n",
    "        month(\"date\").alias(\"month\"),\n",
    "        dayofmonth(\"date\").alias(\"day\"),\n",
    "        dayofweek(\"date\").alias(\"day_of_week\"),\n",
    "        quarter(\"date\").alias(\"quarter\"),\n",
    "        weekofyear(\"date\").alias(\"week_of_year\"),\n",
    "        date_format(\"date\", \"MMMM\").alias(\"month_name\"),\n",
    "        date_format(\"date\", \"EEEE\").alias(\"day_name\"),\n",
    "        when(dayofweek(\"date\").isin([1, 7]), True).otherwise(False).alias(\"is_weekend\"),\n",
    "        date_format(\"date\", \"yyyy-MM\").alias(\"year_month\"),\n",
    "        concat_ws(\"Q\", year(\"date\").cast(\"string\"), quarter(\"date\").cast(\"string\")).alias(\"year_quarter\"),\n",
    "        current_timestamp().alias(\"created_date\"),\n",
    "        current_timestamp().alias(\"modified_date\")\n",
    "    )\n",
    "\n",
    "    return date_dim\n",
    "\n",
    "def create_customer_dimension():\n",
    "    \"\"\"Create customer dimension with person info\"\"\"\n",
    "    logger.info(\"Creating customer dimension...\")\n",
    "\n",
    "    # Read Silver layer data\n",
    "    customer_df = safe_table_read(os.path.join(silver_dir, 'customer'), 'customer')\n",
    "    person_df = safe_table_read(os.path.join(silver_dir, 'person'), 'person')\n",
    "    \n",
    "    if customer_df is None:\n",
    "        logger.error(\"Customer table not found in Silver layer\")\n",
    "        return None\n",
    "    \n",
    "    if person_df is None:\n",
    "        logger.warning(\"Person table not found, creating customer dimension without person details\")\n",
    "        customer_dim = customer_df.select(\n",
    "            col(\"CustomerID\").alias(\"customer_key\"),\n",
    "            col(\"CustomerID\").alias(\"customer_id\"),\n",
    "            col(\"AccountNumber\").alias(\"account_number\"),\n",
    "            lit(\"Unknown\").alias(\"first_name\"),\n",
    "            lit(\"Unknown\").alias(\"middle_name\"),\n",
    "            lit(\"Unknown\").alias(\"last_name\"),\n",
    "            lit(\"Unknown\").alias(\"full_name\"),\n",
    "            lit(\"Unknown\").alias(\"person_type\"),\n",
    "            col(\"TerritoryID\").alias(\"territory_id\"),\n",
    "            current_timestamp().alias(\"created_date\"),\n",
    "            current_timestamp().alias(\"modified_date\")\n",
    "        )\n",
    "    else:\n",
    "        # Join customer with person data\n",
    "        customer_dim = customer_df.join(\n",
    "            person_df,\n",
    "            customer_df.PersonID == person_df.BusinessEntityID,\n",
    "            \"left\"\n",
    "        ).select(\n",
    "            customer_df.CustomerID.alias(\"customer_key\"),\n",
    "            customer_df.CustomerID.alias(\"customer_id\"),\n",
    "            customer_df.AccountNumber.alias(\"account_number\"),\n",
    "            coalesce(person_df.FirstName, lit(\"Unknown\")).alias(\"first_name\"),\n",
    "            coalesce(person_df.MiddleName, lit(\"\")).alias(\"middle_name\"),\n",
    "            coalesce(person_df.LastName, lit(\"Unknown\")).alias(\"last_name\"),\n",
    "            concat(\n",
    "                coalesce(person_df.FirstName, lit(\"Unknown\")), \n",
    "                lit(\" \"),\n",
    "                coalesce(person_df.LastName, lit(\"Unknown\"))\n",
    "            ).alias(\"full_name\"),\n",
    "            coalesce(person_df.PersonType, lit(\"Unknown\")).alias(\"person_type\"),\n",
    "            customer_df.TerritoryID.alias(\"territory_id\"),\n",
    "            current_timestamp().alias(\"created_date\"),\n",
    "            current_timestamp().alias(\"modified_date\")\n",
    "        )\n",
    "\n",
    "    # Add surrogate key\n",
    "    customer_dim = generate_surrogate_key(customer_dim, [\"customer_id\"])\n",
    "    \n",
    "    return customer_dim\n",
    "\n",
    "def create_product_dimension():\n",
    "    \"\"\"Create product dimension with category information\"\"\"\n",
    "    logger.info(\"Creating product dimension...\")\n",
    "\n",
    "    product_df = safe_table_read(os.path.join(silver_dir, 'product'), 'product')\n",
    "    \n",
    "    if product_df is None:\n",
    "        logger.error(\"Product table not found in Silver layer\")\n",
    "        return None\n",
    "\n",
    "    # Try to read additional tables for product categories\n",
    "    product_category_df = safe_table_read(os.path.join(silver_dir, 'productcategory'), 'productcategory')\n",
    "    product_subcategory_df = safe_table_read(os.path.join(silver_dir, 'productsubcategory'), 'productsubcategory')\n",
    "\n",
    "    if product_category_df is not None and product_subcategory_df is not None:\n",
    "        # Join product with category information\n",
    "        product_dim = product_df.alias(\"p\") \\\n",
    "            .join(product_subcategory_df.alias(\"psc\"),\n",
    "                  col(\"p.ProductSubcategoryID\") == col(\"psc.ProductSubcategoryID\"), \"left\") \\\n",
    "            .join(product_category_df.alias(\"pc\"),\n",
    "                  col(\"psc.ProductCategoryID\") == col(\"pc.ProductCategoryID\"), \"left\")\n",
    "\n",
    "        product_dim = product_dim.select(\n",
    "            col(\"p.ProductID\").alias(\"product_key\"),\n",
    "            col(\"p.ProductID\").alias(\"product_id\"),\n",
    "            col(\"p.Name\").alias(\"product_name\"),\n",
    "            col(\"p.ProductNumber\").alias(\"product_number\"),\n",
    "            coalesce(col(\"p.Color\"), lit(\"Unknown\")).alias(\"color\"),\n",
    "            coalesce(col(\"p.Size\"), lit(\"Unknown\")).alias(\"size\"),\n",
    "            coalesce(col(\"p.Weight\"), lit(0.0)).alias(\"weight\"),\n",
    "            coalesce(col(\"p.ListPrice\"), lit(0.0)).alias(\"list_price\"),\n",
    "            coalesce(col(\"p.StandardCost\"), lit(0.0)).alias(\"standard_cost\"),\n",
    "            coalesce(col(\"psc.Name\"), lit(\"Unknown\")).alias(\"subcategory_name\"),\n",
    "            coalesce(col(\"pc.Name\"), lit(\"Unknown\")).alias(\"category_name\"),\n",
    "            current_timestamp().alias(\"created_date\"),\n",
    "            current_timestamp().alias(\"modified_date\")\n",
    "        )\n",
    "    else:\n",
    "        # Create dimension without category information\n",
    "        product_dim = product_df.select(\n",
    "            col(\"ProductID\").alias(\"product_key\"),\n",
    "            col(\"ProductID\").alias(\"product_id\"),\n",
    "            col(\"Name\").alias(\"product_name\"),\n",
    "            col(\"ProductNumber\").alias(\"product_number\"),\n",
    "            coalesce(col(\"Color\"), lit(\"Unknown\")).alias(\"color\"),\n",
    "            coalesce(col(\"Size\"), lit(\"Unknown\")).alias(\"size\"),\n",
    "            coalesce(col(\"Weight\"), lit(0.0)).alias(\"weight\"),\n",
    "            coalesce(col(\"ListPrice\"), lit(0.0)).alias(\"list_price\"),\n",
    "            coalesce(col(\"StandardCost\"), lit(0.0)).alias(\"standard_cost\"),\n",
    "            lit(\"Unknown\").alias(\"subcategory_name\"),\n",
    "            lit(\"Unknown\").alias(\"category_name\"),\n",
    "            current_timestamp().alias(\"created_date\"),\n",
    "            current_timestamp().alias(\"modified_date\")\n",
    "        )\n",
    "\n",
    "    # Add surrogate key\n",
    "    product_dim = generate_surrogate_key(product_dim, [\"product_id\"])\n",
    "\n",
    "    return product_dim\n",
    "\n",
    "def create_geography_dimension():\n",
    "    \"\"\"Create geography dimension\"\"\"\n",
    "    logger.info(\"Creating geography dimension...\")\n",
    "\n",
    "    # Read address data\n",
    "    address_df = safe_table_read(os.path.join(silver_dir, 'address'), 'address')\n",
    "    \n",
    "    if address_df is None:\n",
    "        logger.error(\"Address table not found in Silver layer\")\n",
    "        return None\n",
    "\n",
    "    # Try to read geography reference tables\n",
    "    territory_df = safe_table_read(os.path.join(silver_dir, 'salesterritory'), 'salesterritory')\n",
    "    state_df = safe_table_read(os.path.join(silver_dir, 'stateprovince'), 'stateprovince')\n",
    "    country_df = safe_table_read(os.path.join(silver_dir, 'countryregion'), 'countryregion')\n",
    "\n",
    "    if all([territory_df is not None, state_df is not None, country_df is not None]):\n",
    "        geography_dim = address_df.alias(\"a\") \\\n",
    "            .join(state_df.alias(\"s\"), col(\"a.StateProvinceID\") == col(\"s.StateProvinceID\"), \"left\") \\\n",
    "            .join(country_df.alias(\"c\"), col(\"s.CountryRegionCode\") == col(\"c.CountryRegionCode\"), \"left\") \\\n",
    "            .join(territory_df.alias(\"t\"), col(\"s.TerritoryID\") == col(\"t.TerritoryID\"), \"left\")\n",
    "\n",
    "        geography_dim = geography_dim.select(\n",
    "            col(\"a.AddressID\").alias(\"geography_key\"),\n",
    "            col(\"a.AddressID\").alias(\"address_id\"),\n",
    "            coalesce(col(\"a.AddressLine1\"), lit(\"Unknown\")).alias(\"address_line1\"),\n",
    "            coalesce(col(\"a.AddressLine2\"), lit(\"\")).alias(\"address_line2\"),\n",
    "            coalesce(col(\"a.City\"), lit(\"Unknown\")).alias(\"city\"),\n",
    "            coalesce(col(\"a.PostalCode\"), lit(\"Unknown\")).alias(\"postal_code\"),\n",
    "            coalesce(col(\"s.Name\"), lit(\"Unknown\")).alias(\"state_name\"),\n",
    "            coalesce(col(\"s.StateProvinceCode\"), lit(\"UN\")).alias(\"state_code\"),\n",
    "            coalesce(col(\"c.Name\"), lit(\"Unknown\")).alias(\"country_name\"),\n",
    "            coalesce(col(\"c.CountryRegionCode\"), lit(\"UN\")).alias(\"country_code\"),\n",
    "            coalesce(col(\"t.Name\"), lit(\"Unknown\")).alias(\"territory_name\"),\n",
    "            current_timestamp().alias(\"created_date\"),\n",
    "            current_timestamp().alias(\"modified_date\")\n",
    "        )\n",
    "    else:\n",
    "        # Create simplified geography dimension\n",
    "        geography_dim = address_df.select(\n",
    "            col(\"AddressID\").alias(\"geography_key\"),\n",
    "            col(\"AddressID\").alias(\"address_id\"),\n",
    "            coalesce(col(\"AddressLine1\"), lit(\"Unknown\")).alias(\"address_line1\"),\n",
    "            coalesce(col(\"AddressLine2\"), lit(\"\")).alias(\"address_line2\"),\n",
    "            coalesce(col(\"City\"), lit(\"Unknown\")).alias(\"city\"),\n",
    "            coalesce(col(\"PostalCode\"), lit(\"Unknown\")).alias(\"postal_code\"),\n",
    "            lit(\"Unknown\").alias(\"state_name\"),\n",
    "            lit(\"UN\").alias(\"state_code\"),\n",
    "            lit(\"Unknown\").alias(\"country_name\"),\n",
    "            lit(\"UN\").alias(\"country_code\"),\n",
    "            lit(\"Unknown\").alias(\"territory_name\"),\n",
    "            current_timestamp().alias(\"created_date\"),\n",
    "            current_timestamp().alias(\"modified_date\")\n",
    "        )\n",
    "\n",
    "    # Add surrogate key\n",
    "    geography_dim = generate_surrogate_key(geography_dim, [\"address_id\"])\n",
    "\n",
    "    return geography_dim\n",
    "\n",
    "def create_sales_fact_table():\n",
    "    \"\"\"Create main sales fact table\"\"\"\n",
    "    logger.info(\"Creating sales fact table...\")\n",
    "\n",
    "    # Read Silver layer data\n",
    "    order_header_df = safe_table_read(os.path.join(silver_dir, 'salesorderheader'), 'salesorderheader')\n",
    "    order_detail_df = safe_table_read(os.path.join(silver_dir, 'salesorderdetail'), 'salesorderdetail')\n",
    "\n",
    "    if order_header_df is None or order_detail_df is None:\n",
    "        logger.error(\"Required sales tables not found in Silver layer\")\n",
    "        return None\n",
    "\n",
    "    # Join order header and detail\n",
    "    sales_fact = order_detail_df.alias(\"od\").join(\n",
    "        order_header_df.alias(\"oh\"),\n",
    "        col(\"od.SalesOrderID\") == col(\"oh.SalesOrderID\")\n",
    "    )\n",
    "\n",
    "    # Create fact table with measures and foreign keys\n",
    "    sales_fact = sales_fact.select(\n",
    "        # Surrogate key\n",
    "        monotonically_increasing_id().alias(\"sales_fact_key\"),\n",
    "\n",
    "        # Business keys\n",
    "        col(\"oh.SalesOrderID\").alias(\"sales_order_id\"),\n",
    "        col(\"od.SalesOrderDetailID\").alias(\"sales_order_detail_id\"),\n",
    "\n",
    "        # Foreign keys\n",
    "        col(\"oh.CustomerID\").alias(\"customer_key\"),\n",
    "        col(\"od.ProductID\").alias(\"product_key\"),\n",
    "        coalesce(col(\"oh.BillToAddressID\"), col(\"oh.ShipToAddressID\")).alias(\"bill_to_geography_key\"),\n",
    "        coalesce(col(\"oh.ShipToAddressID\"), col(\"oh.BillToAddressID\")).alias(\"ship_to_geography_key\"),\n",
    "        col(\"oh.OrderDate\").alias(\"order_date_key\"),\n",
    "        coalesce(col(\"oh.DueDate\"), col(\"oh.OrderDate\")).alias(\"due_date_key\"),\n",
    "        coalesce(col(\"oh.ShipDate\"), col(\"oh.OrderDate\")).alias(\"ship_date_key\"),\n",
    "\n",
    "        # Measures\n",
    "        coalesce(col(\"od.OrderQty\"), lit(0)).alias(\"order_quantity\"),\n",
    "        coalesce(col(\"od.UnitPrice\"), lit(0.0)).alias(\"unit_price\"),\n",
    "        coalesce(col(\"od.UnitPriceDiscount\"), lit(0.0)).alias(\"unit_price_discount\"),\n",
    "        coalesce(col(\"od.LineTotal\"), lit(0.0)).alias(\"line_total\"),\n",
    "        coalesce(col(\"oh.SubTotal\"), lit(0.0)).alias(\"order_subtotal\"),\n",
    "        coalesce(col(\"oh.TaxAmt\"), lit(0.0)).alias(\"tax_amount\"),\n",
    "        coalesce(col(\"oh.Freight\"), lit(0.0)).alias(\"freight\"),\n",
    "        coalesce(col(\"oh.TotalDue\"), lit(0.0)).alias(\"total_due\"),\n",
    "\n",
    "        # Calculated measures\n",
    "        (coalesce(col(\"od.OrderQty\"), lit(0)) * coalesce(col(\"od.UnitPrice\"), lit(0.0))).alias(\"gross_revenue\"),\n",
    "        (coalesce(col(\"od.LineTotal\"), lit(0.0))).alias(\"net_revenue\"),\n",
    "\n",
    "        # Attributes\n",
    "        coalesce(col(\"oh.Status\"), lit(0)).alias(\"order_status\"),\n",
    "        coalesce(col(\"oh.OnlineOrderFlag\"), lit(False)).alias(\"online_order_flag\"),\n",
    "\n",
    "        # Audit columns\n",
    "        current_timestamp().alias(\"created_date\"),\n",
    "        current_timestamp().alias(\"modified_date\")\n",
    "    )\n",
    "\n",
    "    return sales_fact\n",
    "\n",
    "def create_comprehensive_revenue_table():\n",
    "    \"\"\"Create comprehensive table with all revenue analysis info merged\"\"\"\n",
    "    logger.info(\"Creating comprehensive revenue analysis table...\")\n",
    "\n",
    "    # Check if all required tables exist\n",
    "    required_tables = ['fact_sales', 'dim_customer', 'dim_product', 'dim_geography', 'dim_date']\n",
    "    missing_tables = []\n",
    "    \n",
    "    for table in required_tables:\n",
    "        table_path = os.path.join(gold_dir, table)\n",
    "        if not os.path.exists(table_path):\n",
    "            missing_tables.append(table)\n",
    "            logger.error(f\"Required table {table} not found at {table_path}\")\n",
    "    \n",
    "    if missing_tables:\n",
    "        logger.error(f\"Cannot create comprehensive revenue table. Missing tables: {missing_tables}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Read fact and dimension tables - get current records only for dimensions\n",
    "        sales_fact = spark.read.parquet(os.path.join(gold_dir, 'fact_sales'))\n",
    "        logger.info(f\"Successfully read sales fact table with {sales_fact.count()} records\")\n",
    "        \n",
    "        # For SCD tables, only get current records for the comprehensive view\n",
    "        # Date dimension doesn't have SCD columns, so read it directly\n",
    "        customer_dim = spark.read.parquet(os.path.join(gold_dir, 'dim_customer')).filter(col(\"is_current\") == True)\n",
    "        logger.info(f\"Successfully read customer dimension with {customer_dim.count()} current records\")\n",
    "        \n",
    "        product_dim = spark.read.parquet(os.path.join(gold_dir, 'dim_product')).filter(col(\"is_current\") == True)\n",
    "        logger.info(f\"Successfully read product dimension with {product_dim.count()} current records\")\n",
    "        \n",
    "        geography_dim = spark.read.parquet(os.path.join(gold_dir, 'dim_geography')).filter(col(\"is_current\") == True)\n",
    "        logger.info(f\"Successfully read geography dimension with {geography_dim.count()} current records\")\n",
    "        \n",
    "        date_dim = spark.read.parquet(os.path.join(gold_dir, 'dim_date'))  # No SCD filter for date dimension\n",
    "        logger.info(f\"Successfully read date dimension with {date_dim.count()} records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading dimension tables: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Create comprehensive table with all dimensions joined\n",
    "    comprehensive_df = sales_fact.alias(\"sf\") \\\n",
    "        .join(customer_dim.alias(\"cd\"), col(\"sf.customer_key\") == col(\"cd.customer_key\"), \"left\") \\\n",
    "        .join(product_dim.alias(\"pd\"), col(\"sf.product_key\") == col(\"pd.product_key\"), \"left\") \\\n",
    "        .join(geography_dim.alias(\"gd\"), col(\"sf.bill_to_geography_key\") == col(\"gd.geography_key\"), \"left\") \\\n",
    "        .join(date_dim.alias(\"dd\"), col(\"sf.order_date_key\") == col(\"dd.date_key\"), \"left\")\n",
    "\n",
    "    # Select all relevant columns for revenue analysis\n",
    "    revenue_analysis_table = comprehensive_df.select(\n",
    "        # Sales metrics\n",
    "        col(\"sf.sales_fact_key\"),\n",
    "        col(\"sf.sales_order_id\"),\n",
    "        col(\"sf.sales_order_detail_id\"),\n",
    "        col(\"sf.order_quantity\"),\n",
    "        col(\"sf.unit_price\"),\n",
    "        col(\"sf.unit_price_discount\"),\n",
    "        col(\"sf.line_total\"),\n",
    "        col(\"sf.net_revenue\"),\n",
    "        col(\"sf.gross_revenue\"),\n",
    "        col(\"sf.order_subtotal\"),\n",
    "        col(\"sf.tax_amount\"),\n",
    "        col(\"sf.freight\"),\n",
    "        col(\"sf.total_due\"),\n",
    "        col(\"sf.order_status\"),\n",
    "        \n",
    "        # Order status description\n",
    "        when(col(\"sf.order_status\") == 1, \"In Process\")\n",
    "        .when(col(\"sf.order_status\") == 2, \"Approved\")\n",
    "        .when(col(\"sf.order_status\") == 3, \"Backordered\")\n",
    "        .when(col(\"sf.order_status\") == 4, \"Rejected\")\n",
    "        .when(col(\"sf.order_status\") == 5, \"Shipped\")\n",
    "        .when(col(\"sf.order_status\") == 6, \"Cancelled\")\n",
    "        .otherwise(\"Unknown\").alias(\"order_status_desc\"),\n",
    "        \n",
    "        col(\"sf.online_order_flag\"),\n",
    "\n",
    "        # Customer information\n",
    "        col(\"cd.customer_id\"),\n",
    "        col(\"cd.account_number\"),\n",
    "        col(\"cd.first_name\"),\n",
    "        col(\"cd.middle_name\"),\n",
    "        col(\"cd.last_name\"),\n",
    "        col(\"cd.full_name\"),\n",
    "        col(\"cd.person_type\"),\n",
    "\n",
    "        # Product information\n",
    "        col(\"pd.product_id\"),\n",
    "        col(\"pd.product_name\"),\n",
    "        col(\"pd.product_number\"),\n",
    "        col(\"pd.color\"),\n",
    "        col(\"pd.size\"),\n",
    "        col(\"pd.weight\"),\n",
    "        col(\"pd.list_price\"),\n",
    "        col(\"pd.standard_cost\"),\n",
    "        col(\"pd.subcategory_name\"),\n",
    "        col(\"pd.category_name\"),\n",
    "\n",
    "        # Geography information\n",
    "        col(\"gd.address_id\"),\n",
    "        col(\"gd.address_line1\"),\n",
    "        col(\"gd.address_line2\"),\n",
    "        col(\"gd.city\"),\n",
    "        col(\"gd.postal_code\"),\n",
    "        col(\"gd.state_name\"),\n",
    "        col(\"gd.state_code\"),\n",
    "        col(\"gd.country_name\"),\n",
    "        col(\"gd.country_code\"),\n",
    "        col(\"gd.territory_name\"),\n",
    "\n",
    "        # Date information\n",
    "        col(\"sf.order_date_key\").alias(\"order_date\"),\n",
    "        col(\"dd.year\").alias(\"order_year\"),\n",
    "        col(\"dd.month\").alias(\"order_month\"),\n",
    "        col(\"dd.quarter\").alias(\"order_quarter\"),\n",
    "        col(\"dd.month_name\"),\n",
    "        col(\"dd.day_name\"),\n",
    "        col(\"dd.year_month\"),\n",
    "        col(\"dd.year_quarter\"),\n",
    "\n",
    "        # Calculated fields for analysis\n",
    "        (col(\"sf.net_revenue\") * col(\"sf.order_quantity\")).alias(\"total_line_revenue\"),\n",
    "        (col(\"pd.list_price\") - col(\"pd.standard_cost\")).alias(\"profit_margin\"),\n",
    "        current_timestamp().alias(\"created_date\")\n",
    "    )\n",
    "\n",
    "    return revenue_analysis_table\n",
    "\n",
    "def write_table_with_logging(df, table_path, table_name, is_dimension=False, business_keys=None):\n",
    "    \"\"\"Write table with logging, error handling, and upsert support\"\"\"\n",
    "    try:\n",
    "        if df is not None:\n",
    "            if is_dimension and business_keys:\n",
    "                # Use SCD Type 2 upsert for dimensions\n",
    "                final_df = upsert_dimension_table(df, table_path, table_name, business_keys)\n",
    "            elif not is_dimension and business_keys:\n",
    "                # Use Type 1 upsert for facts\n",
    "                final_df = upsert_fact_table(df, table_path, table_name, business_keys)\n",
    "            else:\n",
    "                # Default to overwrite for tables without business keys\n",
    "                final_df = df\n",
    "            \n",
    "            record_count = final_df.count()\n",
    "            final_df.write.mode(\"overwrite\").parquet(table_path)\n",
    "            logger.info(f\"{table_name} written successfully with {record_count} records\")\n",
    "        else:\n",
    "            logger.error(f\"Failed to write {table_name} - DataFrame is None\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing {table_name}: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main processing function with SCD and upsert capabilities\"\"\"\n",
    "    logger.info(\"Starting Gold layer processing with SCD and upsert...\")\n",
    "\n",
    "    try:\n",
    "        # Create Date Dimension (no SCD needed for date dimension typically)\n",
    "        logger.info(\"Creating Date Dimension...\")\n",
    "        date_dim = create_date_dimension()\n",
    "        write_table_with_logging(date_dim, os.path.join(gold_dir, 'dim_date'), 'Date Dimension')\n",
    "\n",
    "        # Create Customer Dimension with SCD Type 2\n",
    "        logger.info(\"Creating Customer Dimension...\")\n",
    "        customer_dim = create_customer_dimension()\n",
    "        if customer_dim is not None:\n",
    "            write_table_with_logging(customer_dim, os.path.join(gold_dir, 'dim_customer'), 'Customer Dimension', \n",
    "                                   is_dimension=True, business_keys=['customer_id'])\n",
    "        else:\n",
    "            logger.error(\"Failed to create Customer Dimension\")\n",
    "\n",
    "        # Create Product Dimension with SCD Type 2\n",
    "        logger.info(\"Creating Product Dimension...\")\n",
    "        product_dim = create_product_dimension()\n",
    "        if product_dim is not None:\n",
    "            write_table_with_logging(product_dim, os.path.join(gold_dir, 'dim_product'), 'Product Dimension',\n",
    "                                   is_dimension=True, business_keys=['product_id'])\n",
    "        else:\n",
    "            logger.error(\"Failed to create Product Dimension\")\n",
    "\n",
    "        # Create Geography Dimension with SCD Type 2\n",
    "        logger.info(\"Creating Geography Dimension...\")\n",
    "        geography_dim = create_geography_dimension()\n",
    "        if geography_dim is not None:\n",
    "            write_table_with_logging(geography_dim, os.path.join(gold_dir, 'dim_geography'), 'Geography Dimension',\n",
    "                                   is_dimension=True, business_keys=['address_id'])\n",
    "        else:\n",
    "            logger.error(\"Failed to create Geography Dimension\")\n",
    "\n",
    "        # Create Sales Fact Table with upsert\n",
    "        logger.info(\"Creating Sales Fact Table...\")\n",
    "        sales_fact = create_sales_fact_table()\n",
    "        if sales_fact is not None:\n",
    "            write_table_with_logging(sales_fact, os.path.join(gold_dir, 'fact_sales'), 'Sales Fact Table',\n",
    "                                   is_dimension=False, business_keys=['sales_order_id', 'sales_order_detail_id'])\n",
    "        else:\n",
    "            logger.error(\"Failed to create Sales Fact Table\")\n",
    "\n",
    "        # Create Comprehensive Revenue Analysis Table\n",
    "        logger.info(\"Checking if all tables exist for comprehensive revenue analysis...\")\n",
    "        required_tables = ['fact_sales', 'dim_customer', 'dim_product', 'dim_geography', 'dim_date']\n",
    "        tables_exist = all(os.path.exists(os.path.join(gold_dir, table)) for table in required_tables)\n",
    "        \n",
    "        if tables_exist:\n",
    "            logger.info(\"All required tables exist, creating comprehensive revenue analysis table...\")\n",
    "            revenue_analysis_table = create_comprehensive_revenue_table()\n",
    "            if revenue_analysis_table is not None:\n",
    "                write_table_with_logging(revenue_analysis_table, os.path.join(gold_dir, 'revenue_analysis_comprehensive'), \n",
    "                                       'Comprehensive Revenue Analysis Table')\n",
    "            else:\n",
    "                logger.warning(\"Failed to create comprehensive revenue analysis table\")\n",
    "        else:\n",
    "            missing_tables = [table for table in required_tables if not os.path.exists(os.path.join(gold_dir, table))]\n",
    "            logger.warning(f\"Not all required tables exist, skipping comprehensive revenue table creation. Missing: {missing_tables}\")\n",
    "\n",
    "        logger.info(\"Gold layer processing with SCD and upsert completed successfully!\")\n",
    "\n",
    "        # Display summary statistics\n",
    "        logger.info(\"=== GOLD LAYER SUMMARY ===\")\n",
    "        gold_tables = [d for d in os.listdir(gold_dir) if os.path.isdir(os.path.join(gold_dir, d))]\n",
    "        for table in gold_tables:\n",
    "            try:\n",
    "                df = spark.read.parquet(os.path.join(gold_dir, table))\n",
    "                \n",
    "                # Show SCD info for dimension tables\n",
    "                if table.startswith('dim_') and 'is_current' in df.columns:\n",
    "                    current_count = df.filter(col(\"is_current\") == True).count()\n",
    "                    total_count = df.count()\n",
    "                    logger.info(f\"{table}: {total_count} total records ({current_count} current, {total_count - current_count} historical), {len(df.columns)} columns\")\n",
    "                else:\n",
    "                    logger.info(f\"{table}: {df.count()} records, {len(df.columns)} columns\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error reading {table}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main processing: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logger.error(\"ðŸ”¥ Unhandled exception occurred in main()\", exc_info=True)\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        exit(1)\n",
    "    finally:\n",
    "        spark.stop()\n",
    "        logger.info(\"SparkSession stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7bd8ea",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17562fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Logging Setup ---\n",
    "# Define path for the log file\n",
    "log_dir = 'logs'\n",
    "plots_log_file_path = os.path.join(log_dir, 'plots_generation.log')\n",
    "\n",
    "# Ensure log directory exists\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Clear existing handlers to prevent duplicate logs\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(plots_log_file_path),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Starting Plot Generation for Gold Layer Visualizations...\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "class GoldLayerVisualizer:\n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initialize the visualizer with the path to the merged parquet files\n",
    "        and set up the plot output directory.\n",
    "        \"\"\"\n",
    "        self.data_path = Path(data_path)\n",
    "        self.df = None\n",
    "        self.load_data()\n",
    "\n",
    "        # Determine the base directory of the script\n",
    "        script_dir = Path(__file__).parent\n",
    "        self.base_output_dir = script_dir.parent / 'plots'\n",
    "        \n",
    "        self.obligated_plots_dir = self.base_output_dir / 'obligated_plots'\n",
    "        self.additional_plots_dir = self.base_output_dir / 'additional_plots'\n",
    "        \n",
    "        self.obligated_plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.additional_plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"Obligated plots will be saved to: {self.obligated_plots_dir}\")\n",
    "        logger.info(f\"Additional plots will be saved to: {self.additional_plots_dir}\")\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load the merged parquet data\"\"\"\n",
    "        try:\n",
    "            if self.data_path.is_file():\n",
    "                self.df = pd.read_parquet(self.data_path)\n",
    "            else:\n",
    "                self.df = pd.read_parquet(self.data_path)\n",
    "\n",
    "            logger.info(f\"Data loaded successfully: {len(self.df)} records, {len(self.df.columns)} columns\")\n",
    "\n",
    "            numeric_cols = ['unit_price', 'line_total', 'net_revenue', 'gross_revenue',\n",
    "                           'order_subtotal', 'tax_amount', 'freight', 'total_due',\n",
    "                           'list_price', 'standard_cost', 'weight']\n",
    "\n",
    "            for col in numeric_cols:\n",
    "                if col in self.df.columns:\n",
    "                    if self.df[col].dtype == 'object':\n",
    "                        self.df[col] = self.df[col].astype(str).str.replace(',', '.').astype(float)\n",
    "\n",
    "            if 'order_date' in self.df.columns:\n",
    "                self.df['order_date'] = pd.to_datetime(self.df['order_date'])\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {e}\", exc_info=True)\n",
    "            logger.error(\"Please check the data path and ensure parquet files exist\")\n",
    "            self.df = None # Ensure df is None if loading fails\n",
    "\n",
    "    def _save_plot(self, filename, plot_type='obligated'):\n",
    "        \"\"\"Helper function to save and close the plot.\"\"\"\n",
    "        if plot_type == 'obligated':\n",
    "            file_path = self.obligated_plots_dir / filename\n",
    "        elif plot_type == 'additional':\n",
    "            file_path = self.additional_plots_dir / filename\n",
    "        else:\n",
    "            logger.warning(f\"Unknown plot type '{plot_type}'. Saving to obligated_plots folder.\")\n",
    "            file_path = self.obligated_plots_dir / filename\n",
    "\n",
    "        try:\n",
    "            plt.savefig(file_path)\n",
    "            logger.info(f\"Plot saved successfully: {file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving plot {filename}: {e}\", exc_info=True)\n",
    "        finally:\n",
    "            plt.close()\n",
    "\n",
    "    def plot_revenue_by_category(self):\n",
    "        \"\"\"Plot revenue by all product categories\"\"\"\n",
    "        if self.df is None:\n",
    "            logger.warning(\"DataFrame is None, skipping plot_revenue_by_category.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        category_revenue = self.df.groupby('category_name')['net_revenue'].sum().sort_values(ascending=False)\n",
    "\n",
    "        ax = category_revenue.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "        plt.title('Revenue by Product Category', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Product Category', fontsize=12)\n",
    "        plt.ylabel('Revenue ($)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        for i, v in enumerate(category_revenue.values):\n",
    "            ax.text(i, v + max(category_revenue) * 0.01, f'${v:,.0f}',\n",
    "                   ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        self._save_plot('revenue_by_category.png', plot_type='obligated')\n",
    "\n",
    "    def plot_top_subcategories(self, top_n=10):\n",
    "        \"\"\"Plot top N subcategories by revenue\"\"\"\n",
    "        if self.df is None:\n",
    "            logger.warning(\"DataFrame is None, skipping plot_top_subcategories.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        subcategory_revenue = self.df.groupby('subcategory_name')['net_revenue'].sum().sort_values(ascending=False).head(top_n)\n",
    "\n",
    "        ax = subcategory_revenue.plot(kind='barh', color='lightcoral', edgecolor='black')\n",
    "        plt.title(f'Top {top_n} Subcategories by Revenue', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Revenue ($)', fontsize=12)\n",
    "        plt.ylabel('Product Subcategory', fontsize=12)\n",
    "\n",
    "        for i, v in enumerate(subcategory_revenue.values):\n",
    "            ax.text(v + max(subcategory_revenue) * 0.01, i, f'${v:,.0f}',\n",
    "                   ha='left', va='center', fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        self._save_plot(f'top_{top_n}_subcategories.png', plot_type='obligated')\n",
    "\n",
    "    def plot_top_customers(self, top_n=10):\n",
    "        \"\"\"Plot top N customers by revenue\"\"\"\n",
    "        if self.df is None:\n",
    "            logger.warning(\"DataFrame is None, skipping plot_top_customers.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        customer_revenue = self.df.groupby('full_name')['net_revenue'].sum().sort_values(ascending=False).head(top_n)\n",
    "\n",
    "        ax = customer_revenue.plot(kind='barh', color='lightgreen', edgecolor='black')\n",
    "        plt.title(f'Top {top_n} Customers by Revenue', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Revenue ($)', fontsize=12)\n",
    "        plt.ylabel('Customer Name', fontsize=12)\n",
    "\n",
    "        for i, v in enumerate(customer_revenue.values):\n",
    "            ax.text(v + max(customer_revenue) * 0.01, i, f'${v:,.0f}',\n",
    "                   ha='left', va='center', fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        self._save_plot(f'top_{top_n}_customers.png', plot_type='obligated')\n",
    "\n",
    "    def plot_revenue_by_order_status(self):\n",
    "        \"\"\"Plot revenue by all order statuses\"\"\"\n",
    "        if self.df is None:\n",
    "            logger.warning(\"DataFrame is None, skipping plot_revenue_by_order_status.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        status_revenue = self.df.groupby('order_status_desc')['net_revenue'].sum().sort_values(ascending=False)\n",
    "\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(status_revenue)))\n",
    "        wedges, texts, autotexts = plt.pie(status_revenue.values, labels=status_revenue.index,\n",
    "                                          autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "\n",
    "        plt.title('Revenue Distribution by Order Status', fontsize=16, fontweight='bold')\n",
    "\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_color('white')\n",
    "            autotext.set_fontweight('bold')\n",
    "\n",
    "        plt.axis('equal')\n",
    "        self._save_plot('revenue_by_order_status.png', plot_type='obligated')\n",
    "\n",
    "    def plot_top_countries(self, top_n=10):\n",
    "        \"\"\"Plot top N countries by revenue\"\"\"\n",
    "        if self.df is None:\n",
    "            logger.warning(\"DataFrame is None, skipping plot_top_countries.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        country_revenue = self.df.groupby('country_name')['net_revenue'].sum().sort_values(ascending=False).head(top_n)\n",
    "\n",
    "        ax = country_revenue.plot(kind='bar', color='gold', edgecolor='black')\n",
    "        plt.title(f'Top {top_n} Countries by Revenue', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Country', fontsize=12)\n",
    "        plt.ylabel('Revenue ($)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        for i, v in enumerate(country_revenue.values):\n",
    "            ax.text(i, v + max(country_revenue) * 0.01, f'${v:,.0f}',\n",
    "                   ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        self._save_plot(f'top_{top_n}_countries.png', plot_type='obligated')\n",
    "\n",
    "    def plot_top_states(self, top_n=10):\n",
    "        \"\"\"Plot top N states by revenue\"\"\"\n",
    "        if self.df is None:\n",
    "            logger.warning(\"DataFrame is None, skipping plot_top_states.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        state_revenue = self.df.groupby('state_name')['net_revenue'].sum().sort_values(ascending=False).head(top_n)\n",
    "\n",
    "        ax = state_revenue.plot(kind='barh', color='mediumpurple', edgecolor='black')\n",
    "        plt.title(f'Top {top_n} States by Revenue', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Revenue ($)', fontsize=12)\n",
    "        plt.ylabel('State', fontsize=12)\n",
    "\n",
    "        for i, v in enumerate(state_revenue.values):\n",
    "            ax.text(v + max(state_revenue) * 0.01, i, f'${v:,.0f}',\n",
    "                   ha='left', va='center', fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        self._save_plot(f'top_{top_n}_states.png', plot_type='obligated')\n",
    "\n",
    "    def plot_top_cities(self, top_n=10):\n",
    "        \"\"\"Plot top N cities by revenue\"\"\"\n",
    "        if self.df is None:\n",
    "            logger.warning(\"DataFrame is None, skipping plot_top_cities.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        city_revenue = self.df.groupby('city')['net_revenue'].sum().sort_values(ascending=False).head(top_n)\n",
    "\n",
    "        ax = city_revenue.plot(kind='barh', color='orange', edgecolor='black')\n",
    "        plt.title(f'Top {top_n} Cities by Revenue', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Revenue ($)', fontsize=12)\n",
    "        plt.ylabel('City', fontsize=12)\n",
    "\n",
    "        for i, v in enumerate(city_revenue.values):\n",
    "            ax.text(v + max(city_revenue) * 0.01, i, f'${v:,.0f}',\n",
    "                   ha='left', va='center', fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        self._save_plot(f'top_{top_n}_cities.png', plot_type='obligated')\n",
    "\n",
    "    # Additional insightful plots\n",
    "    def plot_monthly_revenue_trend(self):\n",
    "        \"\"\"Plot monthly revenue trend over time\"\"\"\n",
    "        if self.df is None:\n",
    "            logger.warning(\"DataFrame is None, skipping plot_monthly_revenue_trend.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(16, 8))\n",
    "        self.df['order_date'] = pd.to_datetime(self.df['order_date'])\n",
    "        self.df['order_year'] = self.df['order_date'].dt.year\n",
    "        self.df['order_month'] = self.df['order_date'].dt.month\n",
    "\n",
    "        monthly_revenue = self.df.groupby(['order_year', 'order_month'])['net_revenue'].sum().reset_index()\n",
    "\n",
    "        monthly_revenue['date'] = pd.to_datetime(monthly_revenue['order_year'].astype(str) + '-' +\n",
    "                                                 monthly_revenue['order_month'].astype(str) + '-01')\n",
    "\n",
    "        plt.plot(monthly_revenue['date'], monthly_revenue['net_revenue'],\n",
    "                marker='o', linewidth=2, markersize=6, color='darkblue')\n",
    "        plt.title('Monthly Revenue Trend', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Date', fontsize=12)\n",
    "        plt.ylabel('Revenue ($)', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        z = np.polyfit(range(len(monthly_revenue)), monthly_revenue['net_revenue'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(monthly_revenue['date'], p(range(len(monthly_revenue))),\n",
    "                \"r--\", alpha=0.8, linewidth=2, label=f'Trend Line')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        self._save_plot('monthly_revenue_trend.png', plot_type='additional')\n",
    "\n",
    "    def plot_revenue_by_quarter(self):\n",
    "        \"\"\"Plot revenue by quarter\"\"\"\n",
    "        if self.df is None:\n",
    "            logger.warning(\"DataFrame is None, skipping plot_revenue_by_quarter.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        # Ensure order_quarter is available, if not, create it\n",
    "        if 'order_quarter' not in self.df.columns:\n",
    "            self.df['order_quarter'] = self.df['order_date'].dt.quarter\n",
    "            logger.info(\"Created 'order_quarter' column as it was missing.\")\n",
    "\n",
    "        quarterly_revenue = self.df.groupby('order_quarter')['net_revenue'].sum()\n",
    "\n",
    "        ax = quarterly_revenue.plot(kind='bar', color='teal', edgecolor='black')\n",
    "        plt.title('Revenue by Quarter', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Quarter', fontsize=12)\n",
    "        plt.ylabel('Revenue ($)', fontsize=12)\n",
    "        plt.xticks(rotation=0)\n",
    "\n",
    "        for i, v in enumerate(quarterly_revenue.values):\n",
    "            ax.text(i, v + max(quarterly_revenue) * 0.01, f'${v:,.0f}',\n",
    "                   ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        self._save_plot('revenue_by_quarter.png', plot_type='additional')\n",
    "\n",
    "    def plot_product_performance_matrix(self):\n",
    "        \"\"\"Plot product performance matrix (Revenue vs Quantity)\"\"\"\n",
    "        if self.df is None:\n",
    "            logger.warning(\"DataFrame is None, skipping plot_product_performance_matrix.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(14, 10))\n",
    "\n",
    "        product_perf = self.df.groupby('product_name').agg({\n",
    "            'net_revenue': 'sum',\n",
    "            'order_quantity': 'sum',\n",
    "            'category_name': 'first'\n",
    "        }).reset_index()\n",
    "\n",
    "        categories = product_perf['category_name'].unique()\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, len(categories)))\n",
    "\n",
    "        for i, category in enumerate(categories):\n",
    "            cat_data = product_perf[product_perf['category_name'] == category]\n",
    "            plt.scatter(cat_data['order_quantity'], cat_data['net_revenue'],\n",
    "                        alpha=0.7, s=60, color=colors[i], label=category, edgecolors='black')\n",
    "\n",
    "        plt.title('Product Performance Matrix\\n(Revenue vs Quantity Sold)', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Total Quantity Sold', fontsize=12)\n",
    "        plt.ylabel('Total Revenue ($)', fontsize=12)\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        self._save_plot('product_performance_matrix.png', plot_type='additional')\n",
    "\n",
    "    def plot_customer_type_analysis(self):\n",
    "        \"\"\"Plot customer type analysis\"\"\"\n",
    "        if self.df is None:\n",
    "            logger.warning(\"DataFrame is None, skipping plot_customer_type_analysis.\")\n",
    "            return\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "        customer_type_revenue = self.df.groupby('person_type')['net_revenue'].sum()\n",
    "        ax1.pie(customer_type_revenue.values, labels=customer_type_revenue.index,\n",
    "                autopct='%1.1f%%', startangle=90, colors=plt.cm.Pastel1.colors)\n",
    "        ax1.set_title('Revenue by Customer Type', fontsize=14, fontweight='bold')\n",
    "\n",
    "        avg_order_value = self.df.groupby('person_type')['net_revenue'].mean()\n",
    "        bars = ax2.bar(avg_order_value.index, avg_order_value.values,\n",
    "                       color='lightblue', edgecolor='black')\n",
    "        ax2.set_title('Average Order Value by Customer Type', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Customer Type')\n",
    "        ax2.set_ylabel('Average Order Value ($)')\n",
    "\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + max(avg_order_value) * 0.01,\n",
    "                    f'${height:,.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        self._save_plot('customer_type_analysis.png', plot_type='additional')\n",
    "\n",
    "    def plot_territory_performance(self):\n",
    "        \"\"\"Plot territory performance comparison\"\"\"\n",
    "        if self.df is None:\n",
    "            logger.warning(\"DataFrame is None, skipping plot_territory_performance.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        territory_revenue = self.df.groupby('territory_name')['net_revenue'].sum().sort_values(ascending=False)\n",
    "\n",
    "        ax = territory_revenue.plot(kind='bar', color='darkorange', edgecolor='black')\n",
    "        plt.title('Revenue by Sales Territory', fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Territory', fontsize=12)\n",
    "        plt.ylabel('Revenue ($)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        for i, v in enumerate(territory_revenue.values):\n",
    "            ax.text(i, v + max(territory_revenue) * 0.01, f'${v:,.0f}',\n",
    "                   ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        self._save_plot('territory_performance.png', plot_type='additional')\n",
    "\n",
    "    def generate_summary_stats(self):\n",
    "        \"\"\"Generate and display summary statistics\"\"\"\n",
    "        if self.df is None:\n",
    "            logger.warning(\"DataFrame is None, skipping summary statistics generation.\")\n",
    "            return\n",
    "\n",
    "        logger.info(\"=\" * 50)\n",
    "        logger.info(\"GOLD LAYER DATA SUMMARY STATISTICS\")\n",
    "        logger.info(\"=\" * 50)\n",
    "\n",
    "        logger.info(f\"Total Records: {len(self.df):,}\")\n",
    "        \n",
    "        # Check if 'order_date' column exists and is not empty before calling min/max\n",
    "        if 'order_date' in self.df.columns and not self.df['order_date'].empty:\n",
    "            logger.info(f\"Date Range: {self.df['order_date'].min()} to {self.df['order_date'].max()}\")\n",
    "        else:\n",
    "            logger.warning(\"Order date column is missing or empty, date range not available.\")\n",
    "\n",
    "        logger.info(f\"Total Revenue: ${self.df['net_revenue'].sum():,.2f}\")\n",
    "        logger.info(f\"Average Order Value: ${self.df['net_revenue'].mean():,.2f}\")\n",
    "        logger.info(f\"Total Orders: {self.df['sales_order_id'].nunique():,}\")\n",
    "        logger.info(f\"Unique Customers: {self.df['customer_id'].nunique():,}\")\n",
    "        logger.info(f\"Unique Products: {self.df['product_id'].nunique():,}\")\n",
    "        logger.info(f\"Countries: {self.df['country_name'].nunique()}\")\n",
    "        logger.info(f\"States: {self.df['state_name'].nunique()}\")\n",
    "        logger.info(f\"Cities: {self.df['city'].nunique()}\")\n",
    "\n",
    "        logger.info(\"\\nTop 5 Revenue Generating:\")\n",
    "        # Check if columns exist before grouping\n",
    "        if 'category_name' in self.df.columns and 'net_revenue' in self.df.columns:\n",
    "            logger.info(f\"Categories: {list(self.df.groupby('category_name')['net_revenue'].sum().sort_values(ascending=False).head().index)}\")\n",
    "        else:\n",
    "            logger.warning(\"Category name or net revenue column missing, skipping top categories.\")\n",
    "        \n",
    "        if 'country_name' in self.df.columns and 'net_revenue' in self.df.columns:\n",
    "            logger.info(f\"Countries: {list(self.df.groupby('country_name')['net_revenue'].sum().sort_values(ascending=False).head().index)}\")\n",
    "        else:\n",
    "            logger.warning(\"Country name or net revenue column missing, skipping top countries.\")\n",
    "\n",
    "        if 'state_name' in self.df.columns and 'net_revenue' in self.df.columns:\n",
    "            logger.info(f\"States: {list(self.df.groupby('state_name')['net_revenue'].sum().sort_values(ascending=False).head().index)}\")\n",
    "        else:\n",
    "            logger.warning(\"State name or net revenue column missing, skipping top states.\")\n",
    "\n",
    "    def generate_all_plots(self):\n",
    "        \"\"\"Generate all the requested plots plus additional ones\"\"\"\n",
    "        logger.info(\"Generating comprehensive visualizations...\")\n",
    "\n",
    "        self.generate_summary_stats()\n",
    "\n",
    "        logger.info(\"\\nGenerating Obligated Plots:\")\n",
    "        self.plot_revenue_by_category()\n",
    "        self.plot_top_subcategories(10)\n",
    "        self.plot_top_customers(10)\n",
    "        self.plot_revenue_by_order_status()\n",
    "        self.plot_top_countries(10)\n",
    "        self.plot_top_states(10)\n",
    "        self.plot_top_cities(10)\n",
    "\n",
    "        logger.info(\"\\nGenerating Additional Insightful Plots:\")\n",
    "        self.plot_monthly_revenue_trend()\n",
    "        self.plot_revenue_by_quarter()\n",
    "        self.plot_product_performance_matrix()\n",
    "        self.plot_customer_type_analysis()\n",
    "        self.plot_territory_performance()\n",
    "\n",
    "        logger.info(\"\\nAll visualizations generated successfully!\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    gold_dir = 'data/Gold'\n",
    "    comprehensive_data_path = Path(gold_dir) / 'revenue_analysis_comprehensive'\n",
    "\n",
    "    visualizer = GoldLayerVisualizer(data_path=comprehensive_data_path)\n",
    "    visualizer.generate_all_plots()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
